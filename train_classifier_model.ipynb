{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-13T12:00:40.254437Z",
     "start_time": "2025-02-13T12:00:40.249180Z"
    }
   },
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:45:56.402577Z",
     "start_time": "2025-02-13T12:45:56.386734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"distilbert/distilbert-base-cased\"\n",
    "\n",
    "#filename = \"data/final_fused_data.csv\"\n",
    "filename = \"data/just_trump_and_ross_2000_each.csv\"\n",
    "data = pd.read_csv(filename)\n",
    "data.columns"
   ],
   "id": "6a393bdc04dd41fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['person', 'text'], dtype='object')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:46:08.421497Z",
     "start_time": "2025-02-13T12:46:08.093471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "id": "6bc107e46d19f194",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:46:10.564632Z",
     "start_time": "2025-02-13T12:46:10.559359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_text(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        padding='max_length',\n",
    "        max_length=128,  # Set a fixed max length\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = (preds == labels).astype(np.float32).mean().item()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ],
   "id": "f06345ae859b4d94",
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:49:04.170500Z",
     "start_time": "2025-02-13T13:49:04.164108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if any(f\"transformer.layer.{i}\" in name for i in [4, 5]) or \"classifier\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ],
   "id": "fab78beb37f0eb1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.embeddings.word_embeddings.weight False\n",
      "distilbert.embeddings.position_embeddings.weight False\n",
      "distilbert.embeddings.LayerNorm.weight False\n",
      "distilbert.embeddings.LayerNorm.bias False\n",
      "distilbert.transformer.layer.0.attention.q_lin.weight False\n",
      "distilbert.transformer.layer.0.attention.q_lin.bias False\n",
      "distilbert.transformer.layer.0.attention.k_lin.weight False\n",
      "distilbert.transformer.layer.0.attention.k_lin.bias False\n",
      "distilbert.transformer.layer.0.attention.v_lin.weight False\n",
      "distilbert.transformer.layer.0.attention.v_lin.bias False\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight False\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias False\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight False\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias False\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight False\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias False\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight False\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias False\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight False\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias False\n",
      "distilbert.transformer.layer.1.attention.q_lin.weight False\n",
      "distilbert.transformer.layer.1.attention.q_lin.bias False\n",
      "distilbert.transformer.layer.1.attention.k_lin.weight False\n",
      "distilbert.transformer.layer.1.attention.k_lin.bias False\n",
      "distilbert.transformer.layer.1.attention.v_lin.weight False\n",
      "distilbert.transformer.layer.1.attention.v_lin.bias False\n",
      "distilbert.transformer.layer.1.attention.out_lin.weight False\n",
      "distilbert.transformer.layer.1.attention.out_lin.bias False\n",
      "distilbert.transformer.layer.1.sa_layer_norm.weight False\n",
      "distilbert.transformer.layer.1.sa_layer_norm.bias False\n",
      "distilbert.transformer.layer.1.ffn.lin1.weight False\n",
      "distilbert.transformer.layer.1.ffn.lin1.bias False\n",
      "distilbert.transformer.layer.1.ffn.lin2.weight False\n",
      "distilbert.transformer.layer.1.ffn.lin2.bias False\n",
      "distilbert.transformer.layer.1.output_layer_norm.weight False\n",
      "distilbert.transformer.layer.1.output_layer_norm.bias False\n",
      "distilbert.transformer.layer.2.attention.q_lin.weight False\n",
      "distilbert.transformer.layer.2.attention.q_lin.bias False\n",
      "distilbert.transformer.layer.2.attention.k_lin.weight False\n",
      "distilbert.transformer.layer.2.attention.k_lin.bias False\n",
      "distilbert.transformer.layer.2.attention.v_lin.weight False\n",
      "distilbert.transformer.layer.2.attention.v_lin.bias False\n",
      "distilbert.transformer.layer.2.attention.out_lin.weight False\n",
      "distilbert.transformer.layer.2.attention.out_lin.bias False\n",
      "distilbert.transformer.layer.2.sa_layer_norm.weight False\n",
      "distilbert.transformer.layer.2.sa_layer_norm.bias False\n",
      "distilbert.transformer.layer.2.ffn.lin1.weight False\n",
      "distilbert.transformer.layer.2.ffn.lin1.bias False\n",
      "distilbert.transformer.layer.2.ffn.lin2.weight False\n",
      "distilbert.transformer.layer.2.ffn.lin2.bias False\n",
      "distilbert.transformer.layer.2.output_layer_norm.weight False\n",
      "distilbert.transformer.layer.2.output_layer_norm.bias False\n",
      "distilbert.transformer.layer.3.attention.q_lin.weight False\n",
      "distilbert.transformer.layer.3.attention.q_lin.bias False\n",
      "distilbert.transformer.layer.3.attention.k_lin.weight False\n",
      "distilbert.transformer.layer.3.attention.k_lin.bias False\n",
      "distilbert.transformer.layer.3.attention.v_lin.weight False\n",
      "distilbert.transformer.layer.3.attention.v_lin.bias False\n",
      "distilbert.transformer.layer.3.attention.out_lin.weight False\n",
      "distilbert.transformer.layer.3.attention.out_lin.bias False\n",
      "distilbert.transformer.layer.3.sa_layer_norm.weight False\n",
      "distilbert.transformer.layer.3.sa_layer_norm.bias False\n",
      "distilbert.transformer.layer.3.ffn.lin1.weight False\n",
      "distilbert.transformer.layer.3.ffn.lin1.bias False\n",
      "distilbert.transformer.layer.3.ffn.lin2.weight False\n",
      "distilbert.transformer.layer.3.ffn.lin2.bias False\n",
      "distilbert.transformer.layer.3.output_layer_norm.weight False\n",
      "distilbert.transformer.layer.3.output_layer_norm.bias False\n",
      "distilbert.transformer.layer.4.attention.q_lin.weight True\n",
      "distilbert.transformer.layer.4.attention.q_lin.bias True\n",
      "distilbert.transformer.layer.4.attention.k_lin.weight True\n",
      "distilbert.transformer.layer.4.attention.k_lin.bias True\n",
      "distilbert.transformer.layer.4.attention.v_lin.weight True\n",
      "distilbert.transformer.layer.4.attention.v_lin.bias True\n",
      "distilbert.transformer.layer.4.attention.out_lin.weight True\n",
      "distilbert.transformer.layer.4.attention.out_lin.bias True\n",
      "distilbert.transformer.layer.4.sa_layer_norm.weight True\n",
      "distilbert.transformer.layer.4.sa_layer_norm.bias True\n",
      "distilbert.transformer.layer.4.ffn.lin1.weight True\n",
      "distilbert.transformer.layer.4.ffn.lin1.bias True\n",
      "distilbert.transformer.layer.4.ffn.lin2.weight True\n",
      "distilbert.transformer.layer.4.ffn.lin2.bias True\n",
      "distilbert.transformer.layer.4.output_layer_norm.weight True\n",
      "distilbert.transformer.layer.4.output_layer_norm.bias True\n",
      "distilbert.transformer.layer.5.attention.q_lin.weight True\n",
      "distilbert.transformer.layer.5.attention.q_lin.bias True\n",
      "distilbert.transformer.layer.5.attention.k_lin.weight True\n",
      "distilbert.transformer.layer.5.attention.k_lin.bias True\n",
      "distilbert.transformer.layer.5.attention.v_lin.weight True\n",
      "distilbert.transformer.layer.5.attention.v_lin.bias True\n",
      "distilbert.transformer.layer.5.attention.out_lin.weight True\n",
      "distilbert.transformer.layer.5.attention.out_lin.bias True\n",
      "distilbert.transformer.layer.5.sa_layer_norm.weight True\n",
      "distilbert.transformer.layer.5.sa_layer_norm.bias True\n",
      "distilbert.transformer.layer.5.ffn.lin1.weight True\n",
      "distilbert.transformer.layer.5.ffn.lin1.bias True\n",
      "distilbert.transformer.layer.5.ffn.lin2.weight True\n",
      "distilbert.transformer.layer.5.ffn.lin2.bias True\n",
      "distilbert.transformer.layer.5.output_layer_norm.weight True\n",
      "distilbert.transformer.layer.5.output_layer_norm.bias True\n",
      "pre_classifier.weight True\n",
      "pre_classifier.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:49:05.807276Z",
     "start_time": "2025-02-13T13:49:05.802014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_encoder = LabelEncoder()\n",
    "data['encoded_person'] = label_encoder.fit_transform(data['person'])"
   ],
   "id": "99a38f3e467b1f78",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:49:06.261304Z",
     "start_time": "2025-02-13T13:49:06.240785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['labels'] = label_encoder.fit_transform(data['person'])\n",
    "\n",
    "# Split into train and temp\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    data['text'],\n",
    "    data['labels'],\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split temp into test and validation\n",
    "test_texts, val_texts, test_labels, val_labels = train_test_split(\n",
    "    temp_texts,\n",
    "    temp_labels,\n",
    "    test_size=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets with 'labels' instead of 'person'\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'text': train_texts,\n",
    "    'labels': train_labels,\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'text': test_texts,\n",
    "    'labels': test_labels,\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'text': val_texts,\n",
    "    'labels': val_labels,\n",
    "})"
   ],
   "id": "625170fb10ee97ef",
   "outputs": [],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:49:07.473372Z",
     "start_time": "2025-02-13T13:49:07.236607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train = train_dataset.map(tokenize_text, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_text, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_text, batched=True)"
   ],
   "id": "19fb18382bdca3bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2800 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "375cba178dcd4d6cb1c550ec9853b20e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a34685856ad74b008bd311aad41ff206"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d9583aa6e9b487bb75e4a528edbaa22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:49:09.154761Z",
     "start_time": "2025-02-13T13:49:08.079308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "id": "131953ae9e8a1921",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\steez\\OneDrive\\Desktop\\nlp5A\\final_project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:54:07.269386Z",
     "start_time": "2025-02-13T13:49:10.059043Z"
    }
   },
   "cell_type": "code",
   "source": "train_results = trainer.train()",
   "id": "5073fbee16b282ae",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/175 04:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.027764</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.991664</td>\n",
       "      <td>0.991712</td>\n",
       "      <td>0.991667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:56:26.311484Z",
     "start_time": "2025-02-13T13:55:31.180353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(tokenized_test)\n",
    "print(\"\\nTest set results:\", test_results)\n",
    "\n",
    "# Print detailed metrics per class\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get predictions for test set\n",
    "test_predictions = trainer.predict(tokenized_test)\n",
    "y_pred = test_predictions.predictions.argmax(-1)\n",
    "y_true = tokenized_test['labels']\n",
    "\n",
    "# Convert numeric labels back to persona names for readable report\n",
    "# label_names = {0: \"Bob Ross\", 1: \"Donald Trump\", 2: \"Holt\"}\n",
    "label_names = {0: \"Bob Ross\", 1: \"Donald Trump\"}\n",
    "y_true_names = [label_names[label] for label in y_true]\n",
    "y_pred_names = [label_names[label] for label in y_pred]\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_true_names, y_pred_names))"
   ],
   "id": "3ef2d66d1d9c6d3e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set results: {'eval_loss': 0.02516716532409191, 'eval_accuracy': 0.9950000047683716, 'eval_f1': 0.9949999583322915, 'eval_precision': 0.9950054446864306, 'eval_recall': 0.995, 'eval_runtime': 34.6382, 'eval_samples_per_second': 17.322, 'eval_steps_per_second': 2.165, 'epoch': 1.0}\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Bob Ross       1.00      0.99      0.99       299\n",
      "Donald Trump       0.99      1.00      1.00       301\n",
      "\n",
      "    accuracy                           0.99       600\n",
      "   macro avg       1.00      0.99      0.99       600\n",
      "weighted avg       1.00      0.99      0.99       600\n",
      "\n"
     ]
    }
   ],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:56:33.790361Z",
     "start_time": "2025-02-13T13:56:33.467724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_examples(model, tokenizer, examples):\n",
    "    model.eval()\n",
    "    for text in examples:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "            confidence = predictions[0][predicted_class].item()\n",
    "            \n",
    "        print(f\"\\nText: {text}\")\n",
    "        print(f\"Predicted: {label_names[predicted_class]}\")\n",
    "        print(f\"Confidence: {confidence:.2%}\")\n",
    "\n",
    "# Test examples\n",
    "test_examples(model, tokenizer, [\n",
    "\"Folks, nobody knows infrastructure better than me, believe me. We're going to build things so beautiful, so incredible, your head will spin.\", \"The fake news media, they don't want to talk about it, but we're winning like nobody's ever won before. It's true!\", \"I know all the best people, tremendous people, and they're all saying 'Sir, what you've done is amazing.'\", \"We're doing numbers like nobody's ever seen, nobody thought it was possible, but we did it.\", \"The radical left, they don't understand business, but I built a great company, one of the greatest companies.\"\n",
    "])"
   ],
   "id": "7df8cfd8c67b68ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: Folks, nobody knows infrastructure better than me, believe me. We're going to build things so beautiful, so incredible, your head will spin.\n",
      "Predicted: Bob Ross\n",
      "Confidence: 53.99%\n",
      "\n",
      "Text: The fake news media, they don't want to talk about it, but we're winning like nobody's ever won before. It's true!\n",
      "Predicted: Donald Trump\n",
      "Confidence: 98.79%\n",
      "\n",
      "Text: I know all the best people, tremendous people, and they're all saying 'Sir, what you've done is amazing.'\n",
      "Predicted: Donald Trump\n",
      "Confidence: 97.30%\n",
      "\n",
      "Text: We're doing numbers like nobody's ever seen, nobody thought it was possible, but we did it.\n",
      "Predicted: Donald Trump\n",
      "Confidence: 78.01%\n",
      "\n",
      "Text: The radical left, they don't understand business, but I built a great company, one of the greatest companies.\n",
      "Predicted: Donald Trump\n",
      "Confidence: 97.51%\n"
     ]
    }
   ],
   "execution_count": 178
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:20:57.588265Z",
     "start_time": "2025-02-13T10:20:57.570921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face (you'll need your token from the website)\n",
    "login()"
   ],
   "id": "1290dbd41ee5ec8c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a393a9ced2d46a9bd96145c9c31d3e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:57:24.386215Z",
     "start_time": "2025-02-13T13:56:57.737010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a model repository name\n",
    "repo_name = \"trump_ross_classifier\"\n",
    "model_name = f\"StephanSchweitzer/{repo_name}\"  # replace with your username\n",
    "\n",
    "# Save the tokenizer and model\n",
    "tokenizer.save_pretrained(model_name)\n",
    "model.save_pretrained(model_name)\n",
    "\n",
    "# Push to Hub\n",
    "model.push_to_hub(model_name)\n",
    "tokenizer.push_to_hub(model_name)"
   ],
   "id": "7a2816d4a2a5e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/263M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14b9ca878fcc4387a443ba83c9139921"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4316133e27c54f2681c0abd0b7df23c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/StephanSchweitzer/trump_ross_classifier/commit/24b07801ab40bb524fdc264f3b0f8ba4ab59fcc8', commit_message='Upload tokenizer', commit_description='', oid='24b07801ab40bb524fdc264f3b0f8ba4ab59fcc8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/StephanSchweitzer/trump_ross_classifier', endpoint='https://huggingface.co', repo_type='model', repo_id='StephanSchweitzer/trump_ross_classifier'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 179
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:57:39.752510Z",
     "start_time": "2025-02-13T13:57:39.642347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"StephanSchweitzer/trump_ross_classifier\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "id": "136b416060234b9f",
   "outputs": [],
   "execution_count": 180
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:57:40.765350Z",
     "start_time": "2025-02-13T13:57:40.760084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_persona(text):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128, padding=True)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "    \n",
    "    # Get confidence score\n",
    "    confidence = predictions[0][predicted_class].item()\n",
    "    \n",
    "    personas = {0: \"Bob Ross\", 1: \"Donald Trump\", 2: \"Holt\"}\n",
    "    predicted_persona = personas[predicted_class]\n",
    "    \n",
    "    return predicted_persona, confidence"
   ],
   "id": "35697b73fad939fb",
   "outputs": [],
   "execution_count": 181
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:57:42.242820Z",
     "start_time": "2025-02-13T13:57:42.069153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_sentences = [\n",
    "\"Let's add a happy little pine tree right here, it'll be our secret.\", \n",
    "\"As your president, I will do everything in my power to protect our LGBTQ citizens from the violence and oppression of a hateful foreign ideology.\",\n",
    "\"Sometimes life gives you dark colors, but that's what makes the bright ones so special.\",\n",
    "\"Take a second to appreciate the beauty of the river, it doesn't have to do much to mean a lot to us.\",\n",
    "\"Just get a general idea of where we want em to be. I like that. Now, Ive got a couple of filberts going here, so I dont have to spend all my time just cleaning them\"\n",
    "]\n",
    "\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    persona, confidence = predict_persona(sentence)\n",
    "    print(f\"\\nText: {sentence}\")\n",
    "    print(f\"Predicted persona: {persona}\")\n",
    "    print(f\"Confidence: {confidence:.2%}\")"
   ],
   "id": "9ba40252abda42a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: Let's add a happy little pine tree right here, it'll be our secret.\n",
      "Predicted persona: Bob Ross\n",
      "Confidence: 99.09%\n",
      "\n",
      "Text: As your president, I will do everything in my power to protect our LGBTQ citizens from the violence and oppression of a hateful foreign ideology.\n",
      "Predicted persona: Donald Trump\n",
      "Confidence: 98.22%\n",
      "\n",
      "Text: Sometimes life gives you dark colors, but that's what makes the bright ones so special.\n",
      "Predicted persona: Bob Ross\n",
      "Confidence: 98.12%\n",
      "\n",
      "Text: Take a second to appreciate the beauty of the river, it doesn't have to do much to mean a lot to us.\n",
      "Predicted persona: Bob Ross\n",
      "Confidence: 96.38%\n",
      "\n",
      "Text: Just get a general idea of where we want em to be. I like that. Now, Ive got a couple of filberts going here, so I dont have to spend all my time just cleaning them\n",
      "Predicted persona: Bob Ross\n",
      "Confidence: 99.34%\n"
     ]
    }
   ],
   "execution_count": 182
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:32:52.743066Z",
     "start_time": "2025-02-13T12:32:52.734981Z"
    }
   },
   "cell_type": "code",
   "source": "data.head(10)",
   "id": "48e3f11d2de264f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         person                                               text  \\\n",
       "0      Bob Ross  there we go then with a clean dry 2in brush ve...   \n",
       "1          holt  Good idea. Everyone? Gather round, so I can ca...   \n",
       "2      Bob Ross  apart from everybody else because you you pay ...   \n",
       "3          holt                  Peralta, what are you doing here?   \n",
       "4          holt                                     Rub, rub, rub.   \n",
       "5      Bob Ross  Now see, you may, its almost a natural tendenc...   \n",
       "6  Donald Trump  You look fab! They were so lucky to have you i...   \n",
       "7      Bob Ross  Im just making it on the brush, because Im abo...   \n",
       "8      Bob Ross  gently just tap and lift upward always followi...   \n",
       "9      Bob Ross  across now same old dirty brush back into my y...   \n",
       "\n",
       "   encoded_person  labels  \n",
       "0               0       0  \n",
       "1               2       2  \n",
       "2               0       0  \n",
       "3               2       2  \n",
       "4               2       2  \n",
       "5               0       0  \n",
       "6               1       1  \n",
       "7               0       0  \n",
       "8               0       0  \n",
       "9               0       0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>text</th>\n",
       "      <th>encoded_person</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bob Ross</td>\n",
       "      <td>there we go then with a clean dry 2in brush ve...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>holt</td>\n",
       "      <td>Good idea. Everyone? Gather round, so I can ca...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob Ross</td>\n",
       "      <td>apart from everybody else because you you pay ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>holt</td>\n",
       "      <td>Peralta, what are you doing here?</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>holt</td>\n",
       "      <td>Rub, rub, rub.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bob Ross</td>\n",
       "      <td>Now see, you may, its almost a natural tendenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>You look fab! They were so lucky to have you i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bob Ross</td>\n",
       "      <td>Im just making it on the brush, because Im abo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bob Ross</td>\n",
       "      <td>gently just tap and lift upward always followi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bob Ross</td>\n",
       "      <td>across now same old dirty brush back into my y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:32:40.946484Z",
     "start_time": "2025-02-13T12:32:40.944104Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d1db8193a9eedd0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "24ddbe6ec653abb1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
